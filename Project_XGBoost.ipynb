{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97832174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raghav Kalyanaraman, Chesca Untalan, Enay Bhatnagar\n",
    "\n",
    "# XGBoost and Ensembling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d807f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# Drop the unnecessary columns: 'Id', 'Name', 'Found Location', 'Outcome Time', 'Date of Birth'\n",
    "df = df.drop(columns=['Id', 'Name', 'Found Location', 'Outcome Time', 'Date of Birth'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "816e836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneHotEncodeList = []\n",
    "\n",
    "# Intake Time: \n",
    "\n",
    "# Check for missing values in the 'Intake Time' column\n",
    "# print(df['Intake Time'].isnull().sum()) => 0 missing vals\n",
    "\n",
    "# Convert 'Intake Time' to hour, day of the week, and month columns to be transformed\n",
    "df['hour'] = pd.to_datetime(df['Intake Time']).dt.hour\n",
    "df['dayofweek'] = pd.to_datetime(df['Intake Time']).dt.dayofweek\n",
    "df['month'] = pd.to_datetime(df['Intake Time']).dt.month\n",
    "\n",
    "\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "\n",
    "df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "\n",
    "df['month_sin'] = np.sin(2 * np.pi * (df['month'] - 1) / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * (df['month'] - 1) / 12)\n",
    "\n",
    "# Drop the original 'Intake Time', 'hour', 'dayofweek', and 'month' columns\n",
    "df = df.drop(columns=['Intake Time', 'hour', 'dayofweek', 'month'], axis=1)\n",
    "\n",
    "# Intake Type: \n",
    "# Check for missing values in the 'Intake Type' column\n",
    "# print(df['Intake Type'].isnull().sum()) # => 0 missing vals\n",
    "\n",
    "# Delete the 'Wildlife' records (if any) from the 'Intake Type' column\n",
    "df = df[df['Intake Type'] != 'Wildlife']\n",
    "oneHotEncodeList.append('Intake Type')\n",
    "\n",
    "# Intake Condition:\n",
    "\n",
    "def group_intake_condition(condition):\n",
    "    if pd.isnull(condition):\n",
    "        return 'Other'\n",
    "    condition = condition.lower()\n",
    "    if condition in ['med attn', 'medical', 'med urgent', 'neurologic', 'congenital', 'parvo', 'agonal']:\n",
    "        return 'Medical-related'\n",
    "    elif condition in ['neonatal', 'aged', 'pregnant', 'nursing']:\n",
    "        return 'Life stage'\n",
    "    elif condition in ['normal', 'injured', 'sick']:\n",
    "        return 'Health Status'\n",
    "    elif condition in ['behavior', 'feral']:\n",
    "        return 'Behavioral'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df['Intake Condition'] = df['Intake Condition'].apply(group_intake_condition)\n",
    "\n",
    "oneHotEncodeList.append('Intake Condition')\n",
    "\n",
    "# Animal Type:\n",
    "oneHotEncodeList.append('Animal Type')\n",
    "\n",
    "# Sex upon Intake: Split into two features => Sex and Neutered/Spayed\n",
    "\n",
    "# Check for missing values\n",
    "df['Sex upon Intake'] = df['Sex upon Intake'].fillna('Unknown')\n",
    "# print(df['Sex upon Intake'].isnull().sum())\n",
    "\n",
    "\n",
    "def extract_sex_and_status(sex):\n",
    "    if pd.isnull(sex): return pd.Series([\"Unknown\", \"Unknown\"])\n",
    "    \n",
    "    sex = sex.strip().lower()\n",
    "    if \"neutered\" in sex:\n",
    "        status = \"Neutered\"\n",
    "    elif \"spayed\" in sex:\n",
    "        status = \"Spayed\"\n",
    "    elif \"intact\" in sex:\n",
    "        status = \"Intact\"\n",
    "    else:\n",
    "        status = \"Unknown\"\n",
    "\n",
    "    if \"male\" in sex:\n",
    "        gender = \"Male\"\n",
    "    elif \"female\" in sex:\n",
    "        gender = \"Female\"\n",
    "    else:\n",
    "        gender = \"Unknown\"\n",
    "\n",
    "    return pd.Series([gender, status])\n",
    "\n",
    "df[['Sex', 'Fixed_Status']] = df['Sex upon Intake'].apply(extract_sex_and_status)\n",
    "\n",
    "\n",
    "oneHotEncodeList.append('Sex')\n",
    "oneHotEncodeList.append('Fixed_Status')\n",
    "\n",
    "# Drop original Sex upon Intake\n",
    "df = df.drop('Sex upon Intake', axis=1)\n",
    "\n",
    "# Age upon Intake: Convert to numeric values (in days) and drop the original column\n",
    "\n",
    "# print(df['Age upon Intake'].isnull().sum()) # => 0 missing vals\n",
    "def convert_age_to_days(age_str):\n",
    "    if pd.isnull(age_str):\n",
    "        return np.nan\n",
    "    num, unit = age_str.split()[:2]\n",
    "    num = int(num)\n",
    "    if 'day' in unit:\n",
    "        return num\n",
    "    elif 'week' in unit:\n",
    "        return num * 7\n",
    "    elif 'month' in unit:\n",
    "        return num * 30\n",
    "    elif 'year' in unit:\n",
    "        return num * 365\n",
    "    return np.nan\n",
    "\n",
    "df['Age upon Intake'] = df['Age upon Intake'].apply(convert_age_to_days)\n",
    "df['Age upon Intake'] = df['Age upon Intake'].fillna(df['Age upon Intake'].median())\n",
    "\n",
    "# print(df['Age upon Intake'].isnull().sum()) # => 0 missing vals\n",
    "\n",
    "# Breed:\n",
    "\n",
    "def process_breed(breed):\n",
    "    if pd.isnull(breed):\n",
    "        return pd.Series([\"Unknown\", True]) \n",
    "    \n",
    "    is_mix = \"Mix\" in breed or \"/\" in breed\n",
    "\n",
    "    if \"/\" in breed:\n",
    "        primary = breed.split(\"/\")[0].strip()\n",
    "    else:\n",
    "        primary = breed.replace(\" Mix\", \"\").strip()\n",
    "    return pd.Series([primary, is_mix])\n",
    "\n",
    "df[['Primary_Breed', 'Is_Mix']] = df['Breed'].apply(process_breed)\n",
    "\n",
    "df['Is_Mix'] = df['Is_Mix'].astype(int)\n",
    "\n",
    "vc = df['Primary_Breed'].value_counts()\n",
    "cumulative = vc.cumsum() / vc.sum()\n",
    "top_breeds = cumulative[cumulative <= 0.90].index\n",
    "df['Primary_Breed'] = df['Primary_Breed'].apply(lambda x: x if x in top_breeds else 'Other')\n",
    "oneHotEncodeList.append('Primary_Breed')\n",
    "\n",
    "\n",
    "# Drop the original 'Breed' column\n",
    "df = df.drop(columns=['Breed'], axis=1)\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# ==== START Testing ====\n",
    "# Color: We have 3 potential features to extract from the color column\n",
    "# Base Colors (e.g., black, white, brown)\n",
    "# Patterns (e.g., tabby, brindle, tortie, merle)\n",
    "# Number of colors (solid vs. multi-colored)\n",
    "\n",
    "# color_counter = Counter()\n",
    "# pattern_counter = Counter()\n",
    "# for val in df['Color'].dropna():\n",
    "#     parts = re.split(r'[/ ]+', val)  # splits on '/' and spaces\n",
    "#     for part in parts:\n",
    "#         part_clean = part.strip().title()\n",
    "#         if part_clean: \n",
    "#             color_counter[part_clean] += 1\n",
    "# ==== END Testing ====\n",
    "\n",
    "base_colors = [\n",
    "    'White', 'Black', 'Brown', 'Tan', 'Blue', 'Orange', 'Red', 'Cream', 'Gray',\n",
    "    'Chocolate', 'Yellow', 'Fawn', 'Buff', 'Silver', 'Gold', 'Seal', 'Flame',\n",
    "    'Lilac', 'Apricot', 'Liver', 'Pink', 'Ruddy'\n",
    "]\n",
    "\n",
    "patterns = [\n",
    "    'Tabby', 'Brindle', 'Tricolor', 'Tortie', 'Calico', 'Point',\n",
    "    'Torbie', 'Merle', 'Sable', 'Lynx', 'Tick', 'Smoke', 'Tiger', 'Agouti'\n",
    "]\n",
    "\n",
    "color_groups = {\n",
    "    'Dark': ['Black', 'Chocolate', 'Seal'],\n",
    "    'Light': ['White', 'Cream', 'Buff', 'Silver'],\n",
    "    'Warm': ['Red', 'Orange', 'Flame', 'Gold', 'Apricot'],\n",
    "    'Cool': ['Blue', 'Gray', 'Lilac'],\n",
    "    'Neutral': ['Tan', 'Brown', 'Fawn', 'Yellow', 'Liver', 'Pink', 'Ruddy']\n",
    "}\n",
    "\n",
    "pattern_groups = {\n",
    "    'Striped': ['Tabby', 'Tiger', 'Lynx'],\n",
    "    'Blotched': ['Tortie', 'Calico', 'Torbie'],\n",
    "    'Gradient': ['Smoke', 'Point', 'Sable'],\n",
    "    'Mixed': ['Merle', 'Brindle', 'Tricolor'],\n",
    "    'Textured': ['Tick', 'Agouti'],\n",
    "    'None': []\n",
    "}\n",
    "\n",
    "\n",
    "color_to_group = {c: g for g, clist in color_groups.items() for c in clist}\n",
    "pattern_to_group = {p: g for g, plist in pattern_groups.items() for p in plist}\n",
    "\n",
    "# Group assignment functions\n",
    "def assign_color_group(color_str):\n",
    "    if pd.isnull(color_str): return \"Unknown\"\n",
    "    for part in re.split(r'[/ ]+', color_str):\n",
    "        name = part.strip().title()\n",
    "        if name in color_to_group:\n",
    "            return color_to_group[name]\n",
    "    return \"Other\"\n",
    "\n",
    "def assign_pattern_group(color_str):\n",
    "    if pd.isnull(color_str): return \"None\"\n",
    "    for part in re.split(r'[/ ]+', color_str):\n",
    "        name = part.strip().title()\n",
    "        if name in pattern_to_group:\n",
    "            return pattern_to_group[name]\n",
    "    return \"None\"\n",
    "\n",
    "# Apply to DataFrame\n",
    "df['Color_Group'] = df['Color'].apply(assign_color_group)\n",
    "df['Pattern_Group'] = df['Color'].apply(assign_pattern_group)\n",
    "\n",
    "# Drop the original 'Color' column\n",
    "df = df.drop(columns=['Color'], axis=1)\n",
    "\n",
    "oneHotEncodeList.append('Color_Group')\n",
    "oneHotEncodeList.append('Pattern_Group')\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# df['Primary_Breed'] = le.fit_transform(df['Primary_Breed'])\n",
    "# Label encode all the oneHot encoded columns\n",
    "for col in oneHotEncodeList:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3cd308a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, make_scorer, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "# # Load your dataset preprocess_shelter_data(df, is_train=True, label_encoder=None)\n",
    "\n",
    "X = df.drop(columns=['Outcome Type'])\n",
    "y = df['Outcome Type']\n",
    "y_enc = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_enc, test_size=0.2, stratify=y_enc, random_state=42\n",
    ")\n",
    "\n",
    "# ==== START Testing ====\n",
    "\n",
    "# class_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "# # {'Adoption': 0, 'Died': 1, 'Euthanasia': 2, 'Return to Owner': 3, 'Transfer': 4}\n",
    "\n",
    "# smote_strategy = {\n",
    "#     class_mapping['Died']: 3000,\n",
    "#     class_mapping['Euthanasia']: 5000\n",
    "# }\n",
    "\n",
    "\n",
    "# pipeline = Pipeline([\n",
    "#     (\"smote\", SMOTE(sampling_strategy='minority', random_state=42)),\n",
    "#     (\"xgb\", XGBClassifier(random_state=42, eval_metric='mlogloss'))\n",
    "# ])\n",
    "\n",
    "# param_grid = {\n",
    "#     \"xgb__max_depth\": [8, 9, 10],\n",
    "#     \"xgb__n_estimators\": [150, 200, 250],\n",
    "#     \"xgb__subsample\": [0.7, 0.8, 0.9],\n",
    "# }\n",
    "\n",
    "\n",
    "# cv = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "# grid_search = GridSearchCV(pipeline, param_grid, scoring='f1_macro', cv=5, verbose=1, n_jobs=-1)\n",
    "# grid_search.fit(X, y_enc)\n",
    "\n",
    "# # Best model evaluation\n",
    "# print(\"✅ Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"✅ Best CV Macro F1 Score:\", grid_search.best_score_)\n",
    "\n",
    "\n",
    "# best_xgb: XGBClassifier = grid_search.best_estimator_.named_steps['xgb']\n",
    "\n",
    "# # 3) Define your RF\n",
    "# rf = RandomForestClassifier(\n",
    "#     n_estimators=200,\n",
    "#     min_samples_leaf=2,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "\n",
    "# # 4) Build the stacking classifier\n",
    "# stack = StackingClassifier(\n",
    "#     estimators=[\n",
    "#         ('xgb', best_xgb),\n",
    "#         ('rf', rf)\n",
    "#     ],\n",
    "#     final_estimator=LogisticRegression(max_iter=1000),\n",
    "#     cv=cv,\n",
    "#     n_jobs=-1,\n",
    "#     passthrough=False   # switch to True if you want the meta‐learner to also see original X\n",
    "# )\n",
    "\n",
    "# # 5) Put SMOTE + Stacking in a pipeline\n",
    "# stack_pipeline = Pipeline([\n",
    "#     ('smote', SMOTE(sampling_strategy=smote_strategy, random_state=42)),\n",
    "#     ('stack', stack)\n",
    "# ])\n",
    "\n",
    "# # 6) Fit & evaluate\n",
    "# stack_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # CV performance on training folds\n",
    "# scores = cross_val_score(stack_pipeline, X_train, y_train, cv=cv,\n",
    "#                          scoring='f1_macro', n_jobs=-1)\n",
    "# print(f\"Stack CV f1_macro: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "# # Test‐set performance\n",
    "# y_pred = stack_pipeline.predict(X_test)\n",
    "# print(\"\\n— Test Set Classification Report —\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# print(\"Test Macro F1 Score:\", f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "\n",
    "# Replace SMOTE with SMOTEENN\n",
    "# Use BalancedRandomForestClassifier\n",
    "# MLPClassifier into the stacking ensemble alongside your tuned XGB and BRF.\n",
    "\n",
    "# ==== END Testing ====\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffda2170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Counter({np.int64(0): 44035, np.int64(4): 28019, np.int64(3): 13278, np.int64(2): 2759, np.int64(1): 833})\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raghav\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [22:50:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGB params: {'xgb__max_depth': 9, 'xgb__n_estimators': 150, 'xgb__subsample': 0.7}\n",
      "Best XGB CV F1‑macro: 0.3545507493162973\n",
      "Stack CV f1_macro: 0.3486 ± 0.0032\n",
      "\n",
      "— Test Set Classification Report —\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Adoption       0.63      0.83      0.71     11009\n",
      "           Died       0.04      0.23      0.07       208\n",
      "     Euthanasia       0.00      0.00      0.00       690\n",
      "Return to Owner       0.58      0.41      0.48      3320\n",
      "       Transfer       0.67      0.40      0.50      7005\n",
      "\n",
      "       accuracy                           0.60     22232\n",
      "      macro avg       0.38      0.37      0.35     22232\n",
      "   weighted avg       0.61      0.60      0.58     22232\n",
      "\n",
      "Test Macro F1 Score: 0.35331875274096813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raghav\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Raghav\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\Raghav\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.pipeline import Pipeline  \n",
    "\n",
    "\n",
    "counter = Counter(y_train)\n",
    "print(\"Before:\", counter)\n",
    "\n",
    "smot = SMOTE(\n",
    "    sampling_strategy='minority',\n",
    "    k_neighbors=5,\n",
    "    random_state=42\n",
    ")\n",
    "enn = EditedNearestNeighbours(\n",
    "    sampling_strategy='all',\n",
    "    n_neighbors=3,\n",
    "    kind_sel='all'\n",
    ")\n",
    "\n",
    "# 2) Plug them into SMOTEENN\n",
    "smote_enn = SMOTEENN(\n",
    "    smote=smot,\n",
    "    enn=enn,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4) Retrain/tune your XGB so we can extract the best estimator\n",
    "xgb_pipe = Pipeline([\n",
    "    (\"smoteenn\", smote_enn),\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss'\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"xgb__max_depth\":    [8, 9, 10],\n",
    "    \"xgb__n_estimators\": [150, 200, 250],\n",
    "    \"xgb__subsample\":    [0.7, 0.8, 0.9],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_pipe, param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best XGB params:\", grid_search.best_params_)\n",
    "print(\"Best XGB CV F1‑macro:\", grid_search.best_score_)\n",
    "\n",
    "best_xgb = grid_search.best_estimator_.named_steps['xgb']\n",
    "\n",
    "# 5) Define a balanced RF and a small NN\n",
    "brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(50,),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6) Build the stacking ensemble\n",
    "stack = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('xgb', best_xgb),\n",
    "        ('brf', brf),\n",
    "        ('mlp', mlp)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# 7) Wrap stacking in SMOTEENN pipeline\n",
    "stack_pipeline = Pipeline([\n",
    "    ('smoteenn', smote_enn),\n",
    "    ('stack', stack)\n",
    "])\n",
    "\n",
    "# 8) Fit & evaluate\n",
    "stack_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# CV on training folds\n",
    "cv_scores = cross_val_score(\n",
    "    stack_pipeline, X_train, y_train,\n",
    "    cv=cv, scoring='f1_macro', n_jobs=-1\n",
    ")\n",
    "print(f\"Stack CV f1_macro: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# Hold‑out test\n",
    "y_pred = stack_pipeline.predict(X_test)\n",
    "print(\"\\n— Test Set Classification Report —\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "print(\"Test Macro F1 Score:\", f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b30b76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the training set\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "# best_model = grid_search.best_estimator_\n",
    "# y_pred = best_model.predict(X)\n",
    "\n",
    "# print(\"Training Set Classification Report:\")\n",
    "# print(classification_report(y, y_pred))\n",
    "# print(confusion_matrix(y, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "12be9b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raghav\\AppData\\Local\\Temp\\ipykernel_26340\\1168992652.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_test['hour'] = pd.to_datetime(df_test['Intake Time']).dt.hour\n",
      "C:\\Users\\Raghav\\AppData\\Local\\Temp\\ipykernel_26340\\1168992652.py:15: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_test['dayofweek'] = pd.to_datetime(df_test['Intake Time']).dt.dayofweek\n",
      "C:\\Users\\Raghav\\AppData\\Local\\Temp\\ipykernel_26340\\1168992652.py:16: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df_test['month'] = pd.to_datetime(df_test['Intake Time']).dt.month\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('test.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_test = df_test.drop(columns=['Id', 'Date of Birth', 'Found Location'], axis=1)\n",
    "\n",
    "oneHotEncodeList = []\n",
    "\n",
    "# Intake Time: \n",
    "\n",
    "# Check for missing values in the 'Intake Time' column\n",
    "# print(df_test['Intake Time'].isnull().sum()) => 0 missing vals\n",
    "\n",
    "# Convert 'Intake Time' to hour, day of the week, and month columns to be transformed\n",
    "\n",
    "df_test['hour'] = pd.to_datetime(df_test['Intake Time']).dt.hour\n",
    "df_test['dayofweek'] = pd.to_datetime(df_test['Intake Time']).dt.dayofweek\n",
    "df_test['month'] = pd.to_datetime(df_test['Intake Time']).dt.month\n",
    "\n",
    "# Transform the hour, day of the week, and month columns into sine and cosine features\n",
    "# Can be used for potentially cyclical features\n",
    "df_test['hour_sin'] = np.sin(2 * np.pi * df_test['hour'] / 24)\n",
    "df_test['hour_cos'] = np.cos(2 * np.pi * df_test['hour'] / 24)\n",
    "\n",
    "df_test['dayofweek_sin'] = np.sin(2 * np.pi * df_test['dayofweek'] / 7)\n",
    "df_test['dayofweek_cos'] = np.cos(2 * np.pi * df_test['dayofweek'] / 7)\n",
    "\n",
    "df_test['month_sin'] = np.sin(2 * np.pi * (df_test['month'] - 1) / 12)\n",
    "df_test['month_cos'] = np.cos(2 * np.pi * (df_test['month'] - 1) / 12)\n",
    "\n",
    "# Drop the original 'Intake Time', 'hour', 'dayofweek', and 'month' columns\n",
    "df_test = df_test.drop(columns=['Intake Time', 'hour', 'dayofweek', 'month'], axis=1)\n",
    "\n",
    "# Intake Type: \n",
    "# Check for missing values in the 'Intake Type' column\n",
    "# print(df_test['Intake Type'].isnull().sum()) # => 0 missing vals\n",
    "\n",
    "# Delete the 'Wildlife' records (if any) from the 'Intake Type' column\n",
    "df_test = df_test[df_test['Intake Type'] != 'Wildlife']\n",
    "oneHotEncodeList.append('Intake Type')\n",
    "\n",
    "# Intake Condition:\n",
    "\n",
    "def group_intake_condition(condition):\n",
    "    if pd.isnull(condition):\n",
    "        return 'Other'\n",
    "    condition = condition.lower()\n",
    "    if condition in ['med attn', 'medical', 'med urgent', 'neurologic', 'congenital', 'parvo', 'agonal']:\n",
    "        return 'Medical-related'\n",
    "    elif condition in ['neonatal', 'aged', 'pregnant', 'nursing']:\n",
    "        return 'Life stage'\n",
    "    elif condition in ['normal', 'injured', 'sick']:\n",
    "        return 'Health Status'\n",
    "    elif condition in ['behavior', 'feral']:\n",
    "        return 'Behavioral'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df_test['Intake Condition'] = df_test['Intake Condition'].apply(group_intake_condition)\n",
    "\n",
    "oneHotEncodeList.append('Intake Condition')\n",
    "\n",
    "# Animal Type:\n",
    "oneHotEncodeList.append('Animal Type')\n",
    "\n",
    "# Sex upon Intake: Split into two features => Sex and Neutered/Spayed\n",
    "\n",
    "# Check for missing values\n",
    "df_test['Sex upon Intake'] = df_test['Sex upon Intake'].fillna('Unknown')\n",
    "# print(df_test['Sex upon Intake'].isnull().sum())\n",
    "\n",
    "\n",
    "def extract_sex_and_status(sex):\n",
    "    if pd.isnull(sex): return pd.Series([\"Unknown\", \"Unknown\"])\n",
    "    \n",
    "    sex = sex.strip().lower()\n",
    "    if \"neutered\" in sex:\n",
    "        status = \"Neutered\"\n",
    "    elif \"spayed\" in sex:\n",
    "        status = \"Spayed\"\n",
    "    elif \"intact\" in sex:\n",
    "        status = \"Intact\"\n",
    "    else:\n",
    "        status = \"Unknown\"\n",
    "\n",
    "    if \"male\" in sex:\n",
    "        gender = \"Male\"\n",
    "    elif \"female\" in sex:\n",
    "        gender = \"Female\"\n",
    "    else:\n",
    "        gender = \"Unknown\"\n",
    "\n",
    "    return pd.Series([gender, status])\n",
    "\n",
    "df_test[['Sex', 'Fixed_Status']] = df_test['Sex upon Intake'].apply(extract_sex_and_status)\n",
    "\n",
    "oneHotEncodeList.append('Sex')\n",
    "oneHotEncodeList.append('Fixed_Status')\n",
    "\n",
    "# Drop original Sex upon Intake\n",
    "df_test = df_test.drop('Sex upon Intake', axis=1)\n",
    "\n",
    "# Age upon Intake: Convert to numeric values (in days) and drop the original column\n",
    "\n",
    "# print(df_test['Age upon Intake'].isnull().sum()) # => 0 missing vals\n",
    "def convert_age_to_days(age_str):\n",
    "    if pd.isnull(age_str):\n",
    "        return np.nan\n",
    "    num, unit = age_str.split()[:2]\n",
    "    num = int(num)\n",
    "    if 'day' in unit:\n",
    "        return num\n",
    "    elif 'week' in unit:\n",
    "        return num * 7\n",
    "    elif 'month' in unit:\n",
    "        return num * 30\n",
    "    elif 'year' in unit:\n",
    "        return num * 365\n",
    "    return np.nan\n",
    "\n",
    "df_test['Age upon Intake'] = df_test['Age upon Intake'].apply(convert_age_to_days)\n",
    "df_test['Age upon Intake'] = df_test['Age upon Intake'].fillna(df_test['Age upon Intake'].median())\n",
    "\n",
    "# print(df_test['Age upon Intake'].isnull().sum()) # => 0 missing vals\n",
    "\n",
    "# Breed:\n",
    "\n",
    "def process_breed(breed):\n",
    "    if pd.isnull(breed):\n",
    "        return pd.Series([\"Unknown\", True]) \n",
    "    \n",
    "    is_mix = \"Mix\" in breed or \"/\" in breed\n",
    "\n",
    "    if \"/\" in breed:\n",
    "        primary = breed.split(\"/\")[0].strip()\n",
    "    else:\n",
    "        primary = breed.replace(\" Mix\", \"\").strip()\n",
    "    return pd.Series([primary, is_mix])\n",
    "\n",
    "df_test[['Primary_Breed', 'Is_Mix']] = df_test['Breed'].apply(process_breed)\n",
    "\n",
    "df_test['Is_Mix'] = df_test['Is_Mix'].astype(int)\n",
    "\n",
    "vc = df_test['Primary_Breed'].value_counts()\n",
    "\n",
    "cumulative = vc.cumsum() / vc.sum()\n",
    "top_breeds = cumulative[cumulative <= 0.90].index\n",
    "df_test['Primary_Breed'] = df_test['Primary_Breed'].apply(lambda x: x if x in top_breeds else 'Other')\n",
    "\n",
    "# Drop the original 'Breed' column\n",
    "df_test = df_test.drop(columns=['Breed'], axis=1)\n",
    "oneHotEncodeList.append('Primary_Breed')\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "# Color: We have 3 potential features to extract from the color column\n",
    "# Base Colors (e.g., black, white, brown)\n",
    "# Patterns (e.g., tabby, brindle, tortie, merle)\n",
    "# Number of colors (solid vs. multi-colored)\n",
    "\n",
    "# color_counter = Counter()\n",
    "# pattern_counter = Counter()\n",
    "# for val in df_test['Color'].dropna():\n",
    "#     parts = re.split(r'[/ ]+', val)  # splits on '/' and spaces\n",
    "#     for part in parts:\n",
    "#         part_clean = part.strip().title()\n",
    "#         if part_clean: \n",
    "#             color_counter[part_clean] += 1\n",
    "\n",
    "base_colors = [\n",
    "    'White', 'Black', 'Brown', 'Tan', 'Blue', 'Orange', 'Red', 'Cream', 'Gray',\n",
    "    'Chocolate', 'Yellow', 'Fawn', 'Buff', 'Silver', 'Gold', 'Seal', 'Flame',\n",
    "    'Lilac', 'Apricot', 'Liver', 'Pink', 'Ruddy'\n",
    "]\n",
    "\n",
    "patterns = [\n",
    "    'Tabby', 'Brindle', 'Tricolor', 'Tortie', 'Calico', 'Point',\n",
    "    'Torbie', 'Merle', 'Sable', 'Lynx', 'Tick', 'Smoke', 'Tiger', 'Agouti'\n",
    "]\n",
    "\n",
    "color_groups = {\n",
    "    'Dark': ['Black', 'Chocolate', 'Seal'],\n",
    "    'Light': ['White', 'Cream', 'Buff', 'Silver'],\n",
    "    'Warm': ['Red', 'Orange', 'Flame', 'Gold', 'Apricot'],\n",
    "    'Cool': ['Blue', 'Gray', 'Lilac'],\n",
    "    'Neutral': ['Tan', 'Brown', 'Fawn', 'Yellow', 'Liver', 'Pink', 'Ruddy']\n",
    "}\n",
    "\n",
    "pattern_groups = {\n",
    "    'Striped': ['Tabby', 'Tiger', 'Lynx'],\n",
    "    'Blotched': ['Tortie', 'Calico', 'Torbie'],\n",
    "    'Gradient': ['Smoke', 'Point', 'Sable'],\n",
    "    'Mixed': ['Merle', 'Brindle', 'Tricolor'],\n",
    "    'Textured': ['Tick', 'Agouti'],\n",
    "    'None': []\n",
    "}\n",
    "\n",
    "\n",
    "color_to_group = {c: g for g, clist in color_groups.items() for c in clist}\n",
    "pattern_to_group = {p: g for g, plist in pattern_groups.items() for p in plist}\n",
    "\n",
    "# Group assignment functions\n",
    "def assign_color_group(color_str):\n",
    "    if pd.isnull(color_str): return \"Unknown\"\n",
    "    for part in re.split(r'[/ ]+', color_str):\n",
    "        name = part.strip().title()\n",
    "        if name in color_to_group:\n",
    "            return color_to_group[name]\n",
    "    return \"Other\"\n",
    "\n",
    "def assign_pattern_group(color_str):\n",
    "    if pd.isnull(color_str): return \"None\"\n",
    "    for part in re.split(r'[/ ]+', color_str):\n",
    "        name = part.strip().title()\n",
    "        if name in pattern_to_group:\n",
    "            return pattern_to_group[name]\n",
    "    return \"None\"\n",
    "\n",
    "# Apply to DataFrame\n",
    "df_test['Color_Group'] = df_test['Color'].apply(assign_color_group)\n",
    "df_test['Pattern_Group'] = df_test['Color'].apply(assign_pattern_group)\n",
    "\n",
    "# Drop the original 'Color' column\n",
    "df_test = df_test.drop(columns=['Color'], axis=1)\n",
    "\n",
    "oneHotEncodeList.append('Color_Group')\n",
    "oneHotEncodeList.append('Pattern_Group')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# df_test['Primary_Breed'] = le.fit_transform(df_test['Primary_Breed'])\n",
    "# Label encode all the oneHot encoded columns\n",
    "for col in oneHotEncodeList:\n",
    "    le = LabelEncoder()\n",
    "    df_test[col] = le.fit_transform(df_test[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca0ec34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test Set Predictions Saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Try the stacked model on the unlabeled test set\n",
    "\n",
    "y_pred_test = stack_pipeline.predict(df_test)\n",
    "y_pred_test = label_encoder.inverse_transform(y_pred_test)\n",
    "df_test['Outcome Type'] = y_pred_test\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'Id': data['Id'],\n",
    "    'Outcome Type': y_pred_test\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Test Set Predictions Saved to submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
